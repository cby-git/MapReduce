package com.bw.demo.wordcount;


import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;
/**
 * reducer
 * 在MR中起到归并作用，就是需要汇总出所有的map的输出结果
 * 4个泛型
 * KEYIN，VALUEIN：输入中的<K,V>必须与map中的输出k，v保持一致
 * KEYOUT：就是最终显示的k类型
 * VALUEOUT：最终显示的v类型
 * @author asus
 *
 */
public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

	public void reduce(Text key, Iterable<IntWritable> values,Context context)
			throws IOException, InterruptedException {
		// TODO Auto-generated method stub
		int count = 0;
		//values就是<k,v>中的v 是一个出现次数的集合[1,1,1,1,1]
		for (IntWritable value : values) {
			//count++  每次直接+1
			//get方法就是把value从hadoop io的数据类型转换成int类型
			count += value.get();
		}
		//输出结果
		context.write(key, new IntWritable(count));
	}
}
