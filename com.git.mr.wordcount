package com.bw.demo.wordcount;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

/**
 * mapper
 * 负责从文件中一行一行读取单词 并根据给定的分隔符分割单词
 * 在对每个单词进行处理
 * 
 * 
 * mapper类要求指定的4个泛型
 * 
 * 由于java的数据类型在完成对象的序列化与反序列化效率较低，
 * 所以hadoop单独准备了一套数据类型
 * long---LongWritable
 * string----Text
 * int---IntWritable
 * 
 * 
 * KEYIN, 每次要读取文件的一行数据，指的是输入<key,value>中的key的数据类型
 * keyin的位置就是数据文件中每行数据的偏移量
 * VALUEIN, 就是数据文件中每行数据
 * KEYOUT, 输出时k的类型
 * VALUEOUT, 输出时v的类型
 * 
 * 
 * @author asus
 *
 */
public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
	//
	public void map(LongWritable key,Text value,Context context) throws IOException, InterruptedException{
		//1.把每个单词从hadoop的Text类型转换成java字符串类型
		String words = value.toString();
		//2.把1中的一行单词按照空格分割开
		String[] wordarr = words.split(" ");
		//遍历数组wordrr 每个单词没出现一次就计数1
		for (String word : wordarr) {
			context.write(new Text(word), new IntWritable(1));  //输出<k,v>
		/**
		 * 规则：按什么统计什么
		 * 按照什么：这个map输出的key
		 * 统计什么：这个map输出的value
		 */
		}
	}
}
